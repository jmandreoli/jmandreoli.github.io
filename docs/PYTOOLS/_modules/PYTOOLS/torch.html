<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>PYTOOLS.torch &mdash; PYTOOLS  documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            PYTOOLS
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../mod_init.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS</span></code> (top level) — Generic utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_torch.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.torch</span></code> — Utilities for pytorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_animation.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.animation</span></code> — Animation utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_cache.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.cache</span></code> — A persistent cache mechanism</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_ipywidgets.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.ipywidgets</span></code> — Widget utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_polling.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.polling</span></code> — Generic polling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_simpy.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.simpy</span></code> — Utilities for simpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_experiment.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.experiment</span></code> — Utilities for pytorch experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mod_tensorboard_manager.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">PYTOOLS.tensorboard_manager</span></code> — A server of tensorboard servers</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">PYTOOLS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Module code</a></li>
          <li class="breadcrumb-item"><a href="../PYTOOLS.html">PYTOOLS</a></li>
      <li class="breadcrumb-item active">PYTOOLS.torch</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for PYTOOLS.torch</h1><div class="highlight"><pre>
<span></span><span class="c1"># File:                 torch.py</span>
<span class="c1"># Creation date:        2020-01-15</span>
<span class="c1"># Contributors:         Jean-Marc Andreoli</span>
<span class="c1"># Language:             python</span>
<span class="c1"># Purpose:              Some utilities for pytorch</span>
<span class="c1">#</span>

<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Sequence</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Generator</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Any</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">logging</span><span class="p">;</span> <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="c1">#==================================================================================================</span>
<div class="viewcode-block" id="GeneralisedConvolution">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.GeneralisedConvolution">[docs]</a>
<span class="k">class</span> <span class="nc">GeneralisedConvolution</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">An instance of this class is a generalised convolution module. Ref:</span>

<span class="sd">  Andreoli, Jean-Marc. 2019. ‘`Convolution, Attention and Structure Embedding &lt;https://arxiv.org/abs/1905.01289&gt;`_’. In Proc. of NeurIPS Workshop on Graph Representation Learning, 8. Vancouver, BC, Canada.</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="c1">#==================================================================================================</span>

  <span class="n">temperature</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Temperature of the softmax applied to scores&quot;&quot;&quot;</span>
  <span class="n">K</span><span class="p">:</span><span class="nb">int</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Number of heads&quot;&quot;&quot;</span>
  <span class="n">P</span><span class="p">:</span><span class="nb">int</span><span class="p">;</span> <span class="n">Q</span><span class="p">:</span><span class="nb">int</span><span class="p">;</span> <span class="n">D</span><span class="p">:</span><span class="nb">int</span>
  <span class="n">projx</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span><span class="p">;</span> <span class="n">projy</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">K</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="n">P</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="n">Q</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">D</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Model parameters are</span>

<span class="sd">.. math::</span>

<span class="sd">   \Theta^{\textrm{(x)}}{:}\langle K,P,D \rangle\; \Theta^{\textrm{(y)}}{:}\langle K,Q,D \rangle\; \eta{:}\langle K,D \rangle\; \eta^{\textrm{(o)}}{:}\langle Q \rangle</span>

<span class="sd">:param K: number of heads</span>
<span class="sd">:param P: input dimension :math:`P`</span>
<span class="sd">:param Q: output dimension :math:`Q`, default :math:`P`</span>
<span class="sd">:param D: convolution head dimension :math:`D`, default :math:`\lfloor\frac{Q}{K}\rfloor`</span>
<span class="sd">:param bias: whether to use the biases :math:`\eta,\eta^{\textrm{(o)}}`</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">K</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">K</span><span class="p">);</span> <span class="n">P</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">P</span><span class="p">);</span> <span class="n">Q</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span><span class="n">P</span><span class="p">);</span> <span class="n">D</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">D</span><span class="p">,</span><span class="n">Q</span><span class="o">//</span><span class="n">K</span><span class="p">);</span> <span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">D</span><span class="o">=</span><span class="n">K</span><span class="p">,</span><span class="n">P</span><span class="p">,</span><span class="n">Q</span><span class="p">,</span><span class="n">D</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projx</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;kpd,bmp-&gt;bkmd&#39;</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">P</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span> <span class="c1"># ϴx,η</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projy</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;kqd,bknd-&gt;bnq&#39;</span><span class="p">,</span><span class="n">K</span><span class="p">,</span><span class="n">Q</span><span class="p">,</span><span class="n">D</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span> <span class="c1"># ϴy,ηₒ</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="GeneralisedConvolution.forward">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.GeneralisedConvolution.forward">[docs]</a>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">score</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">x</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">mask</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">process_attn</span><span class="p">:</span><span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span><span class="n">Any</span><span class="p">]</span><span class="o">=</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span><span class="kc">None</span><span class="p">))</span><span class="o">-&gt;</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Generalised Convolution formula (with biases):</span>

<span class="sd">.. math::</span>

<span class="sd">   \begin{align*}</span>
<span class="sd">   y_b &amp; = \sum_k A_{bk}^\top\bar{x}_{bk}\Theta_k^{\textrm{(y)}\top}+\mathbf{1}_N\otimes\eta^{\textrm{(o)}}\\</span>
<span class="sd">   \textrm{where } &amp; A_{bk}{:}\langle M,N \rangle\; \bar{x}_{bk}{:}\langle M,D \rangle\\</span>
<span class="sd">   A_{bk} &amp; = \textrm{softmax}_{\textrm{col}}\frac{1}{\textrm{temp}}(\bar{A}_{bk}+\textrm{mask}_b)\\</span>
<span class="sd">   \bar{x}_{bk} &amp;= x_b \Theta_k^{\textrm{(x)}}+\mathbf{1}_M\otimes\eta_k</span>
<span class="sd">   \end{align*}</span>

<span class="sd">:param score: tensor :math:`\bar{A}{:}\langle B,K,M,N \rangle` (attention scores)</span>
<span class="sd">:param x: tensor :math:`x{:}\langle B,M,P \rangle` (input)</span>
<span class="sd">:param mask: tensor :math:`\langle B,M,N \rangle` or :math:`\langle M,N \rangle` (in log domain: possible values include :math:`-\infty`)</span>
<span class="sd">:param process_attn: applied to the attention tensor :math:`\langle B,K,M,N \rangle` (caution: large) to produce the auxiliary output; default returns :const:`None`</span>
<span class="sd">:return: pair of output tensor :math:`y{:}\langle B,N,Q \rangle` and auxiliary output (see *process_attn*)</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">score</span> <span class="c1"># r: B,K,M,N</span>
    <span class="k">del</span> <span class="n">score</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># mask: M,N or B,M,N</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span> <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">mask</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">,:,:]</span>
      <span class="k">else</span><span class="p">:</span> <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">;</span> <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">mask</span><span class="p">[:,</span><span class="kc">None</span><span class="p">,:,:]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">r</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">temperature</span><span class="p">,</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># attention tensor</span>
    <span class="n">aux</span> <span class="o">=</span> <span class="n">process_attn</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="n">x_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projx</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># x_: B,K,M,D</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bkmn,bkmd-&gt;bknd&#39;</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">x_</span><span class="p">)</span>  <span class="c1"># r: B,K,N,D</span>
    <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projy</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="c1"># r: B,N,Q</span>
    <span class="k">return</span> <span class="n">r</span><span class="p">,</span><span class="n">aux</span></div>


<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">_dim</span><span class="p">(</span><span class="n">d</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span><span class="n">default</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">int</span><span class="p">:</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">if</span> <span class="n">d</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="k">assert</span> <span class="n">default</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">;</span> <span class="k">return</span> <span class="n">default</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="n">d</span><span class="o">&lt;=</span><span class="mi">1</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;dim&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">d</span></div>


<span class="c1">#==================================================================================================</span>
<div class="viewcode-block" id="GeneralisedMultiHeadAttention">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.GeneralisedMultiHeadAttention">[docs]</a>
<span class="k">class</span> <span class="nc">GeneralisedMultiHeadAttention</span><span class="p">(</span><span class="n">GeneralisedConvolution</span><span class="p">):</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This class regroups a number of subclasses in which the scores are computed from projections of the inputs.</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="c1">#==================================================================================================</span>

  <span class="n">Pʹ</span><span class="p">:</span><span class="nb">int</span><span class="p">;</span> <span class="n">Qʹ</span><span class="p">:</span><span class="nb">int</span><span class="p">;</span> <span class="n">Dʹ</span><span class="p">:</span><span class="nb">int</span>
  <span class="n">projxʹ</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span><span class="p">;</span> <span class="n">projyʹ</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">Pʹ</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">Qʹ</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">Dʹ</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">_bias</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Additional model parameters for generalised multi-head attention are:</span>

<span class="sd">.. math::</span>

<span class="sd">   \Lambda^{\textrm{(x)}}{:}\langle K,P&#39;,D&#39; \rangle\; \Lambda^{\textrm{(y)}}{:}\langle K,Q&#39;,D&#39; \rangle\; \beta^{\textrm{(x)}},\beta^{\textrm{(y)}}{:}\langle K,D&#39; \rangle</span>

<span class="sd">:param Pʹ: key-input dimension :math:`P&#39;`, default :math:`P`</span>
<span class="sd">:param Qʹ: query-input dimension :math:`Q&#39;`, default :math:`Q`</span>
<span class="sd">:param Dʹ: attention head dimension :math:`D&#39;`, default :math:`\lfloor\frac{Q&#39;}{K}\rfloor`</span>
<span class="sd">:param bias: whether to use the bias :math:`\beta^{\textrm{(x),(y)}}` + the convolution biases</span>
<span class="sd">:param args: passed to :class:`GeneralisedConvolution` constructor</span>
<span class="sd">:param kargs: passed to :class:`GeneralisedConvolution` constructor</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span>
    <span class="n">Pʹ</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Pʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">P</span><span class="p">);</span> <span class="n">Qʹ</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Qʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">);</span> <span class="n">Dʹ</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Dʹ</span><span class="p">,</span><span class="n">Qʹ</span><span class="o">//</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">);</span> <span class="bp">self</span><span class="o">.</span><span class="n">Pʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Qʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Dʹ</span><span class="o">=</span><span class="n">Pʹ</span><span class="p">,</span><span class="n">Qʹ</span><span class="p">,</span><span class="n">Dʹ</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projxʹ</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;kpd,bmp-&gt;bkmd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Pʹ</span><span class="p">,</span><span class="n">Dʹ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="p">(</span><span class="n">bias</span> <span class="k">if</span> <span class="n">_bias</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">_bias</span><span class="p">))</span> <span class="c1"># Λx,βx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projyʹ</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;kqd,bnq-&gt;bknd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Qʹ</span><span class="p">,</span><span class="n">Dʹ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span> <span class="c1"># Λy,βy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">Dʹ</span><span class="p">))</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="GeneralisedMultiHeadAttention.forward">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.GeneralisedMultiHeadAttention.forward">[docs]</a>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">yʹ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">xʹ</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">x</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">ctx</span><span class="p">:</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">=</span><span class="p">(),</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Uses generalised attention scores obtained from projection of the (key and query) inputs.</span>

<span class="sd">  .. math::</span>

<span class="sd">   \begin{align*}</span>
<span class="sd">   &amp; \bar{x}_{bk}{:}\langle M,D&#39; \rangle,\; \bar{y}_{bk}{:}\langle N,D&#39; \rangle\\</span>
<span class="sd">   \bar{x}_{bk} &amp; = x&#39;_b\Lambda_k^{\textrm{(x)}}+\mathbf{1}_M\otimes\beta^{\textrm{(x)}}_k\\</span>
<span class="sd">   \bar{y}_{bk} &amp; = y&#39;_b\Lambda_k^{\textrm{(y)}}+\mathbf{1}_N\otimes\beta^{\textrm{(y)}}_k</span>
<span class="sd">   \end{align*}</span>

<span class="sd">:param yʹ: tensor :math:`y&#39;{:}\langle B,N,Q&#39; \rangle` (query-input)</span>
<span class="sd">:param xʹ: tensor :math:`x&#39;{:}\langle B,M,P&#39; \rangle` (key-input), default :math:`y&#39;`</span>
<span class="sd">:param x: passed to :meth:`GeneralisedConvolution.forward` (value-input), default :math:`x&#39;`</span>
<span class="sd">:param ctx: context for scoring</span>
<span class="sd">:param kargs: passed to :meth:`GeneralisedConvolution.forward`</span>
<span class="sd">:return: see :meth:`GeneralisedConvolution.forward`</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">if</span> <span class="n">xʹ</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">xʹ</span> <span class="o">=</span> <span class="n">yʹ</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">x</span> <span class="o">=</span> <span class="n">xʹ</span>
    <span class="n">x̄</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projxʹ</span><span class="p">(</span><span class="n">xʹ</span><span class="p">)</span> <span class="c1"># x_: B,K,M,Dʹ</span>
    <span class="n">ȳ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projyʹ</span><span class="p">(</span><span class="n">yʹ</span><span class="p">)</span> <span class="c1"># y_: B,K,N,Dʹ</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x̄</span><span class="p">,</span><span class="n">ȳ</span><span class="p">,</span><span class="o">*</span><span class="n">ctx</span><span class="p">),</span><span class="n">x</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span></div>
</div>


<span class="c1">#==================================================================================================</span>
<div class="viewcode-block" id="MultiHeadAttention">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadAttention">[docs]</a>
<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">GeneralisedMultiHeadAttention</span><span class="p">):</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">An instance of this class is a Vanilla multi-head attention module. Ref:</span>

<span class="sd">  Vaswani et al. 2017. ‘`Attention Is All You Need &lt;http://arxiv.org/abs/1706.03762&gt;`_’. arXiv: 1706.03762</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="c1">#==================================================================================================</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Same as :class:`GeneralisedMultiHeadAttention` constructor, but disables bias :math:`\beta^{\textrm{(x)}}` which is redundant.</span>

<span class="sd">:param args: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">:param kargs: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadAttention.score">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadAttention.score">[docs]</a>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="n">x̄</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">ȳ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Computes the attention scores used in :meth:`forward`. Conforms to :class:`torch.nn.MultiheadAttention`</span>

<span class="sd">:param x̄: tensor :math:`\bar{x}{:}\langle B,K,M,D&#39; \rangle` (key-proj)</span>
<span class="sd">:param ȳ: tensor :math:`\bar{y}{:}\langle B,K,N,D&#39; \rangle` (query-proj)</span>
<span class="sd">:return: tensor :math:`\langle B,K,M,N \rangle`</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bkmd,bknd-&gt;bkmn&#39;</span><span class="p">,</span><span class="n">x̄</span><span class="p">,</span><span class="n">ȳ</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span></div>


<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadAttention.torch_convert">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadAttention.torch_convert">[docs]</a>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">torch_convert</span><span class="p">(</span><span class="n">a</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">tuple</span><span class="p">[</span><span class="s1">&#39;MultiHeadAttention&#39;</span><span class="p">,</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span><span class="nb">int</span><span class="p">,</span><span class="nb">int</span><span class="p">],</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Converts a :class:`torch.nn.MultiheadAttention` instance into an instance of this class with same behaviour (up to numerical instabilities). An additional output is a function :func:`test` which draws a random sample of the module input given batch size and two sequence lengths, and returns a comparison of the two modules on that sample. Example::</span>

<span class="sd">   mod = torch.nn.MultiheadAttention(embed_dim=12,num_heads=4,batch_first=True,kdim=17,vdim=19)</span>
<span class="sd">   mod_,test = MultiHeadAttention.torch_convert(mod)</span>
<span class="sd">   cmp = test(B=64,M=100,N=80) # generates sample(batch:64,in-length:100,out-length:80) and returns aligned comparisons</span>
<span class="sd">   assert all(torch.allclose(u,v,atol=1e-5) for u,v in cmp.values()) # increase atol if it fails</span>

<span class="sd">:param a: the instance to convert</span>
<span class="sd">:return: an equivalent :class:`MultiHeadAttention` instance and its test function</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">def</span> <span class="nf">set_data</span><span class="p">():</span>
      <span class="n">q_weight_d</span><span class="p">,</span><span class="n">k_weight_d</span><span class="p">,</span><span class="n">v_weight_d</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">q_proj_weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">k_proj_weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">v_proj_weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">in_proj_weight</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
      <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projyʹ</span><span class="p">,</span><span class="n">q_weight_d</span><span class="o">.</span><span class="n">T</span><span class="p">),(</span><span class="bp">self</span><span class="o">.</span><span class="n">projxʹ</span><span class="p">,</span><span class="n">k_weight_d</span><span class="o">.</span><span class="n">T</span><span class="p">),(</span><span class="bp">self</span><span class="o">.</span><span class="n">projx</span><span class="p">,</span><span class="n">v_weight_d</span><span class="o">.</span><span class="n">T</span><span class="p">),(</span><span class="bp">self</span><span class="o">.</span><span class="n">projy</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">proj</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="n">L</span><span class="p">:</span> <span class="n">proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q_bias_d</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">v_bias_d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">data</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># ignore useless k_bias_d</span>
        <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">projyʹ</span><span class="p">,</span><span class="n">q_bias_d</span><span class="p">),(</span><span class="bp">self</span><span class="o">.</span><span class="n">projx</span><span class="p">,</span><span class="n">v_bias_d</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">proj</span><span class="p">,</span><span class="n">b</span> <span class="ow">in</span> <span class="n">L</span><span class="p">:</span> <span class="n">proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">num_heads</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projy</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span>
    <span class="k">def</span> <span class="nf">get_grad</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">Generator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]:</span>
      <span class="n">q_weight_g</span><span class="p">,</span><span class="n">k_weight_g</span><span class="p">,</span><span class="n">v_weight_g</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">q_proj_weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">k_proj_weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">v_proj_weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span> <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">in_proj_weight</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">in_proj_weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
      <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;Λy&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projyʹ</span><span class="p">,</span><span class="n">q_weight_g</span><span class="o">.</span><span class="n">T</span><span class="p">),(</span><span class="s1">&#39;Λx&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projxʹ</span><span class="p">,</span><span class="n">k_weight_g</span><span class="o">.</span><span class="n">T</span><span class="p">),(</span><span class="s1">&#39;ϴx&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projx</span><span class="p">,</span><span class="n">v_weight_g</span><span class="o">.</span><span class="n">T</span><span class="p">),(</span><span class="s1">&#39;ϴy&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projy</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
      <span class="k">yield from</span> <span class="p">((</span><span class="n">p</span><span class="p">,</span><span class="n">proj</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">proj</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="n">L</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">q_bias_g</span><span class="p">,</span><span class="n">_</span><span class="p">,</span><span class="n">v_bias_g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">in_proj_bias</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># ignore useless k_bias_g (theoretically always null)</span>
        <span class="n">L</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;βy&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projyʹ</span><span class="p">,</span><span class="n">q_bias_g</span><span class="p">),(</span><span class="s1">&#39;η&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projx</span><span class="p">,</span><span class="n">v_bias_g</span><span class="p">)</span>
        <span class="k">yield from</span> <span class="p">((</span><span class="n">p</span><span class="p">,</span><span class="n">proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,</span><span class="mi">0</span><span class="p">,:],</span><span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)))</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">proj</span><span class="p">,</span><span class="n">b</span> <span class="ow">in</span> <span class="n">L</span><span class="p">)</span>
        <span class="k">yield</span> <span class="s1">&#39;ηₒ&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">projy</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">a</span><span class="o">.</span><span class="n">out_proj</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">),</span> <span class="s1">&#39;Argument must be a torch.nn.MultiheadAttention instance&#39;</span>
    <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">batch_first</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;Option batch_first=False not supported (too lazy although easy)&#39;</span>
    <span class="k">assert</span> <span class="n">a</span><span class="o">.</span><span class="n">bias_k</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">bias_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;Extra biases not supported (no idea what they do)&#39;</span>
    <span class="bp">self</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">K</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span><span class="n">P</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">vdim</span><span class="p">,</span><span class="n">Q</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">Pʹ</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">kdim</span><span class="p">,</span><span class="n">Qʹ</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">D</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">Dʹ</span> <span class="o">==</span> <span class="n">a</span><span class="o">.</span><span class="n">head_dim</span> <span class="c1"># sanity check</span>
    <span class="n">set_data</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">B</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="n">M</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="n">N</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
      <span class="n">yʹ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">N</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>  <span class="c1"># query</span>
      <span class="n">xʹ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">M</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">kdim</span><span class="p">)</span>  <span class="c1"># key</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">M</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">vdim</span><span class="p">)</span>  <span class="c1"># value</span>
      <span class="n">y</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">yʹ</span><span class="p">,</span><span class="n">xʹ</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">need_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">);</span> <span class="n">y_</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">yʹ</span><span class="p">,</span><span class="n">xʹ</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="c1"># forward</span>
      <span class="n">a</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">();</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># reset all gradients</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">();</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># back-propagation</span>
      <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;&#39;</span><span class="p">:(</span><span class="n">y</span><span class="p">,</span><span class="n">y_</span><span class="p">)}</span><span class="o">|</span><span class="p">{</span><span class="n">p</span><span class="p">:(</span><span class="n">u</span><span class="p">,</span><span class="n">u_</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">u_</span><span class="p">,</span><span class="n">u</span> <span class="ow">in</span> <span class="n">get_grad</span><span class="p">()}</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">,</span><span class="n">test</span></div>
</div>


<span class="c1">#==================================================================================================</span>
<div class="viewcode-block" id="MultiHeadMixedAttention">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttention">[docs]</a>
<span class="k">class</span> <span class="nc">MultiHeadMixedAttention</span><span class="p">(</span><span class="n">GeneralisedMultiHeadAttention</span><span class="p">):</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">An instance of this class is a multi-head Mixed attention module, inspired by various papers in the literature.</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="c1">#==================================================================================================</span>

  <span class="n">Rʹ</span><span class="p">:</span><span class="nb">int</span>
  <span class="n">projzx</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span><span class="p">;</span> <span class="n">projzy</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Additional model parameters for Mixed attention scores are:</span>

<span class="sd">.. math::</span>

<span class="sd">   \begin{array}{l}</span>
<span class="sd">   \Lambda^{\textrm{(zx)}},\Lambda^{\textrm{(zy)}}{:}\langle K,R&#39;,D&#39; \rangle</span>
<span class="sd">   \end{array}</span>

<span class="sd">:param Rʹ: edge-input dimension :math:`R&#39;`</span>
<span class="sd">:param args: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">:param kargs: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span> <span class="c1"># Λx,βx  Λy,βy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Rʹ</span><span class="o">=</span><span class="n">Rʹ</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Rʹ</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projzx</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;krd,bmnr-&gt;bkmnd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Dʹ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Λzx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projzy</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;krd,bmnr-&gt;bkmnd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Dʹ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Λzy</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadMixedAttention.forward">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttention.forward">[docs]</a>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">zʹ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Invokes :meth:`GeneralisedMultiHeadAttention.forward` with edge input as scoring context.</span>

<span class="sd">:param zʹ: tensor :math:`z&#39;{:}\langle B,M,N,R&#39; \rangle` (edge-input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">ctx</span><span class="o">=</span><span class="p">(</span><span class="n">zʹ</span><span class="p">,),</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span></div>


<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadMixedAttention.score">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttention.score">[docs]</a>
  <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x̄</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">ȳ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">zʹ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Computes the attention scores passed to generalised convolution:</span>

<span class="sd">.. math::</span>

<span class="sd">   \begin{align*}</span>
<span class="sd">   \bar{A}_{bk} &amp; = E_{\frac{nmd,mnd}{mn}}(\;\mathbf{1}_N{\otimes}\bar{x}_{bk}{+}E_{\frac{mnr,rd}{nmd}}(z&#39;_b,\Lambda_k^{\textrm{(zx)}})\;,\;\mathbf{1}_M{\otimes}\bar{y}_{bk}{+}E_{\frac{mnr,rd}{mnd}}(z&#39;_b,\Lambda_k^{\textrm{(zy)}})\;)</span>
<span class="sd">   \end{align*}</span>

<span class="sd">:param x̄: tensor :math:`\bar{x}{:}\langle B,K,M,D&#39; \rangle` (key-proj)</span>
<span class="sd">:param ȳ: tensor :math:`\bar{y}{:}\langle B,K,N,D&#39; \rangle` (query-proj)</span>
<span class="sd">:param zʹ: tensor :math:`z&#39;{:}\langle B,M,N,R&#39; \rangle` (edge-input)</span>
<span class="sd">:return: see :meth:`GeneralisedMultiHeadAttention.score`</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="n">z̄</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projzx</span><span class="p">(</span><span class="n">zʹ</span><span class="p">)</span> <span class="c1"># z̄: B,K,M,N,D&#39;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bkmd,bkmnd-&gt;bkmn&#39;</span><span class="p">,</span><span class="n">x̄</span><span class="p">,</span><span class="n">z̄</span><span class="p">)</span>
    <span class="n">z̄</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projzy</span><span class="p">(</span><span class="n">zʹ</span><span class="p">)</span> <span class="c1"># z̄: B,K,M,N,D&#39;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bknd,bkmnd-&gt;bkmn&#39;</span><span class="p">,</span><span class="n">ȳ</span><span class="p">,</span><span class="n">z̄</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span></div>
</div>


<span class="c1">#==================================================================================================</span>
<div class="viewcode-block" id="MultiHeadMixedAttentionAlt1">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttentionAlt1">[docs]</a>
<span class="k">class</span> <span class="nc">MultiHeadMixedAttentionAlt1</span><span class="p">(</span><span class="n">GeneralisedMultiHeadAttention</span><span class="p">):</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">An instance of this class is a multi-head Mixed attention module. Ref:</span>

<span class="sd">  Henderson et al. 2023. ‘`Transformers as Graph-to-Graph Models &lt;https://doi.org/10.18653/v1/2023.bigpicture-1.8&gt;`_’. In Proc. of the Big Picture Workshop, 93–107. Singapore: Association for Computational Linguistics.</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="c1">#==================================================================================================</span>

  <span class="n">Rʹ</span><span class="p">:</span><span class="nb">int</span>
  <span class="n">projzx</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span><span class="p">;</span> <span class="n">projzy</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Additional model parameters for Mixed attention scores are:</span>

<span class="sd">.. math::</span>

<span class="sd">   \begin{array}{l}</span>
<span class="sd">   \Lambda^{\textrm{(zx)}},\Lambda^{\textrm{(zy)}}{:}\langle K,R&#39;,D&#39; \rangle</span>
<span class="sd">   \end{array}</span>

<span class="sd">:param Rʹ: edge-input dimension :math:`R&#39;`</span>
<span class="sd">:param args: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">:param kargs: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span> <span class="c1"># Λx,βx  Λy,βy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Rʹ</span><span class="o">=</span><span class="n">Rʹ</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Rʹ</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projzx</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;krd,bmnr-&gt;bkmnd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Dʹ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Λzx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projzy</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;krd,bmnr-&gt;bkmnd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">Dʹ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Λzy</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadMixedAttentionAlt1.forward">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttentionAlt1.forward">[docs]</a>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">zʹ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Invokes :meth:`GeneralisedMultiHeadAttention.forward` with edge input as scoring context.</span>

<span class="sd">:param zʹ: tensor :math:`z&#39;{:}\langle B,M,N,R&#39; \rangle` (edge-input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">ctx</span><span class="o">=</span><span class="p">(</span><span class="n">zʹ</span><span class="p">,),</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span></div>


<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadMixedAttentionAlt1.score">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttentionAlt1.score">[docs]</a>
  <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x̄</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">ȳ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">zʹ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Computes the attention scores passed to generalised convolution:</span>

<span class="sd">.. math::</span>

<span class="sd">   \begin{align*}</span>
<span class="sd">   \bar{A}_{bk} &amp; = \bar{x}_{bk}\bar{y}_{bk}^\top+E_{\frac{md,mnr,rd}{mn}}(\bar{x}_{bk},z&#39;_b,\Lambda_k^{\textrm{(zy)}})+E_{\frac{nd,mnr,rd}{mn}}(\bar{y}_{bk},z&#39;_b,\Lambda_k^{\textrm{(zx)}})</span>
<span class="sd">   \end{align*}</span>

<span class="sd">:param x̄: tensor :math:`\bar{x}{:}\langle B,K,M,D&#39; \rangle` (key-proj)</span>
<span class="sd">:param ȳ: tensor :math:`\bar{y}{:}\langle B,K,N,D&#39; \rangle` (query-proj)</span>
<span class="sd">:param zʹ: tensor :math:`z&#39;{:}\langle B,M,N,R&#39; \rangle` (edge-input)</span>
<span class="sd">:return: see :meth:`GeneralisedMultiHeadAttention.score`</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bkmd,bknd-&gt;bkmn&#39;</span><span class="p">,</span><span class="n">x̄</span><span class="p">,</span><span class="n">ȳ</span><span class="p">)</span> <span class="c1"># standard attention, r: B,K,M,N</span>
    <span class="n">z̄</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projzx</span><span class="p">(</span><span class="n">zʹ</span><span class="p">)</span> <span class="c1"># z̄: B,K,M,N,D&#39;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bknd,bkmnd-&gt;bkmn&#39;</span><span class="p">,</span><span class="n">ȳ</span><span class="p">,</span><span class="n">z̄</span><span class="p">)</span>
    <span class="n">z̄</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projzy</span><span class="p">(</span><span class="n">zʹ</span><span class="p">)</span> <span class="c1"># z̄: B,K,M,N,D&#39;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bkmd,bkmnd-&gt;bkmn&#39;</span><span class="p">,</span><span class="n">x̄</span><span class="p">,</span><span class="n">z̄</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r</span></div>
</div>


<span class="c1">#==================================================================================================</span>
<div class="viewcode-block" id="MultiHeadMixedAttentionAlt2">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttentionAlt2">[docs]</a>
<span class="k">class</span> <span class="nc">MultiHeadMixedAttentionAlt2</span><span class="p">(</span><span class="n">GeneralisedMultiHeadAttention</span><span class="p">):</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">An instance of this class is a multi-head Mixed attention module. Ref:</span>

<span class="sd">  Kwon et al. 2023. ‘`Matrix Encoding Networks for Neural Combinatorial Optimization &lt;https://arxiv.org/abs/2106.11113&gt;`_’. In Proc. of 37-th Annual Conference on Neural Information Processing Systems (NeurIPS), New Orleans, LA, U.S.A.</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="c1">#==================================================================================================</span>

  <span class="n">Rʹ</span><span class="p">:</span><span class="nb">int</span><span class="p">;</span> <span class="n">Dʺ</span><span class="p">:</span><span class="nb">int</span>
  <span class="n">projzʹ</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span><span class="p">;</span> <span class="n">proj1</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span><span class="p">;</span> <span class="n">proj2</span><span class="p">:</span><span class="s1">&#39;Einsum&#39;</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">Dʺ</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">mlpd</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Additional model parameters for Mixed attention scores are:</span>

<span class="sd">.. math::</span>

<span class="sd">   \begin{array}{l}</span>
<span class="sd">   \Lambda^{\textrm{(z)}}{:}\langle K,R&#39;,D&#39;&#39; \rangle\; \alpha,\alpha&#39;{:}\langle K,D&#39;&#39; \rangle</span>
<span class="sd">   \end{array}</span>

<span class="sd">:param Rʹ: edge-input dimension :math:`R&#39;`</span>
<span class="sd">:param Dʺ: internal multi-layer perceptron dimension :math:`D&#39;&#39;` (default see below)</span>
<span class="sd">:param mlpd: the default value for :math:`D&#39;&#39;` is given by :math:`\lfloor R&#39;{\times}\textrm{mlpd}\rfloor`</span>
<span class="sd">:param args: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">:param kargs: passed to :class:`GeneralisedMultiHeadAttention` constructor</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span> <span class="c1"># Λx,βx  Λy,βy</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">Rʹ</span><span class="o">=</span><span class="n">Rʹ</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Rʹ</span><span class="p">);</span> <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mlpd</span><span class="p">,</span><span class="nb">float</span><span class="p">);</span> <span class="bp">self</span><span class="o">.</span><span class="n">Dʺ</span><span class="o">=</span><span class="n">Dʺ</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_dim</span><span class="p">(</span><span class="n">Dʺ</span><span class="p">,</span><span class="nb">int</span><span class="p">(</span><span class="n">mlpd</span><span class="o">*</span><span class="n">Rʹ</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">projzʹ</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;krd,bmnr-&gt;bkmnd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Rʹ</span><span class="p">,</span><span class="n">Dʺ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Λz</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proj1</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;kd,bkmn-&gt;bkmnd&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Dʺ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># α</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">proj2</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;kd,bkmnd-&gt;bkmn&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">K</span><span class="p">,</span><span class="n">Dʺ</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># α&#39;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">)</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadMixedAttentionAlt2.forward">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttentionAlt2.forward">[docs]</a>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">zʹ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">Any</span><span class="p">]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Invokes :meth:`GeneralisedMultiHeadAttention.forward` with edge input as scoring context.</span>

<span class="sd">:param zʹ: tensor :math:`z&#39;{:}\langle B,M,N,R&#39; \rangle` (edge-input)</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="n">ctx</span><span class="o">=</span><span class="p">(</span><span class="n">zʹ</span><span class="p">,),</span><span class="o">**</span><span class="n">kargs</span><span class="p">)</span></div>


<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="MultiHeadMixedAttentionAlt2.score">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.MultiHeadMixedAttentionAlt2.score">[docs]</a>
  <span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x̄</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">ȳ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">zʹ</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Computes the attention scores passed to generalised convolution:</span>

<span class="sd">.. math::</span>

<span class="sd">   \begin{align*}</span>
<span class="sd">   \bar{A}_{bk} &amp; = \textrm{relu}(\bar{x}_{bk}\bar{y}_{bk}^\top\otimes\alpha_k+z&#39;_b\Lambda_k^{\textrm{(z)}})\alpha&#39;_k</span>
<span class="sd">   \end{align*}</span>

<span class="sd">:param x̄: tensor :math:`\bar{x}{:}\langle B,K,M,D&#39; \rangle` (key-proj)</span>
<span class="sd">:param ȳ: tensor :math:`\bar{y}{:}\langle B,K,N,D&#39; \rangle` (query-proj)</span>
<span class="sd">:param zʹ: tensor :math:`z&#39;{:}\langle B,M,N,R&#39; \rangle` (edge-input)</span>
<span class="sd">:return: see :meth:`GeneralisedMultiHeadAttention.score`</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;bkmd,bknd-&gt;bkmn&#39;</span><span class="p">,</span><span class="n">x̄</span><span class="p">,</span><span class="n">ȳ</span><span class="p">)</span> <span class="c1"># r: B,K,M,N</span>
    <span class="n">z̄</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projzʹ</span><span class="p">(</span><span class="n">zʹ</span><span class="p">)</span> <span class="c1"># z̄: B,K,M,N,D&#39;&#39;</span>
    <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj1</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="n">z̄</span> <span class="c1"># r: B,K,M,N,D&#39;&#39;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj2</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="c1"># r: B,K,M,N</span>
    <span class="k">return</span> <span class="n">r</span></div>
</div>


<span class="c1">#==================================================================================================</span>
<div class="viewcode-block" id="Einsum">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.Einsum">[docs]</a>
<span class="k">class</span> <span class="nc">Einsum</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">An instance of this class is essentially a Linear module allowing more flexibility in indices using the einsum notation.</span>
<span class="sd">  &quot;&quot;&quot;</span>
<span class="c1">#==================================================================================================</span>
  <span class="n">sig_pattern</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;([a-z]+),([a-z]+)-&gt;([a-z]+)&#39;</span><span class="p">)</span>
<span class="w">  </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pattern for allowed signatures (the 3 groups specify the weight, input and output components)&quot;&quot;&quot;</span>

  <span class="n">weight</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">;</span> <span class="n">bias</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">sig</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span><span class="o">*</span><span class="n">dims</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="n">bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Model parameters are</span>

<span class="sd">* a weight tensor :math:`\Lambda` matching the weight component of *sig* and</span>
<span class="sd">* a bias tensor :math:`\beta` matching the intersection of the output component and the weight component of *sig*.</span>

<span class="sd">For example with :code:`Einsum(&#39;pq,bnp-&gt;bnq&#39;,P,Q)` where :code:`P,Q` are integers, the model parameters are:</span>

<span class="sd">.. math::</span>

<span class="sd">   \Lambda{:}\langle P,Q\rangle\; \beta{:}\langle Q\rangle</span>

<span class="sd">:param sig: an einsum-like signature conforming to :attr:`sig_pattern`</span>
<span class="sd">:param dims: shape of the weight tensor</span>
<span class="sd">:param bias: whether to use a bias</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sig</span><span class="p">,</span><span class="n">bias_dims</span> <span class="o">=</span> <span class="n">sig</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">_parse</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span><span class="n">dims</span><span class="p">,</span><span class="n">bias</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span>  <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="o">*</span><span class="n">dims</span><span class="p">))</span> <span class="c1"># Λ</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">bias_dims</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="o">*</span><span class="n">bias_dims</span><span class="p">))</span> <span class="c1"># β</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reset_params</span><span class="p">()</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="nf">_reset_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="Einsum.forward">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.Einsum.forward">[docs]</a>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Computes :math:`y` defined as follows:</span>

<span class="sd">.. math::</span>

<span class="sd">   y = E_{\textrm{sig}}(\Lambda,x) + \textrm{reshape}(\beta)</span>

<span class="sd">where :math:`E_{\textrm{sig}}` denotes the Einsum operator with the signature given by attribute :attr:`sig`, and parameter :math:`\beta` is reshaped to match the output component of the signature (in the example, it becomes :math:`\mathbf{1}_B\otimes\mathbf{1}_N\otimes\beta` where dimensions :math:`B,N` come from the input :math:`x`).</span>

<span class="sd">:param x: tensor :math:`x` matching the input component of the signature (in the example :math:`x{:}\langle B,N,P\rangle`)</span>
<span class="sd">:return: tensor :math:`y` matching the output component of the signature (in the example :math:`y{:}\langle B,N,Q\rangle`)</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sig</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">r</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">return</span> <span class="n">r</span></div>


<span class="c1">#--------------------------------------------------------------------------------------------------</span>
  <span class="k">def</span> <span class="nf">_parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">sig</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span><span class="n">dims</span><span class="p">:</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span><span class="n">bias</span><span class="p">:</span><span class="nb">bool</span><span class="p">):</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">for</span> <span class="n">culprit</span><span class="p">,</span><span class="n">check</span> <span class="ow">in</span> <span class="p">((</span><span class="s1">&#39;signature&#39;</span><span class="p">,</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">sig</span><span class="p">,</span><span class="nb">str</span><span class="p">)),(</span><span class="s1">&#39;dims&#39;</span><span class="p">,</span><span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">dims</span><span class="p">)),(</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="nb">bool</span><span class="p">))):</span>
      <span class="k">if</span> <span class="n">check</span> <span class="ow">is</span> <span class="kc">False</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">culprit</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sig_pattern</span><span class="o">.</span><span class="n">fullmatch</span><span class="p">(</span><span class="n">sig</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">m</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Signature must conform to &quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">sig_pattern</span><span class="si">}</span><span class="s1">&quot;&#39;</span>
    <span class="n">p_</span><span class="p">,</span><span class="n">i_</span><span class="p">,</span><span class="n">o_</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">groups</span><span class="p">()</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">p_</span><span class="p">))</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">o_</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">o_</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">all</span><span class="p">(</span><span class="n">c</span> <span class="ow">in</span> <span class="n">p_</span> <span class="ow">or</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">i_</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">o_</span><span class="p">),</span> <span class="s1">&#39;Wrong signature&#39;</span>
    <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
      <span class="n">bias_</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">o_</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">p_</span><span class="p">]</span>
      <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">bias_</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;The given signature does not allow bias; set bias=False&#39;</span>
      <span class="n">dims_</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span><span class="n">dims</span><span class="p">[</span><span class="n">p_</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">c</span><span class="p">)]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">bias_</span><span class="p">}</span>
      <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">dims_</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">o_</span><span class="p">)</span>

<span class="c1">#--------------------------------------------------------------------------------------------------</span>
<div class="viewcode-block" id="Einsum.torch_convert">
<a class="viewcode-back" href="../../mod_torch.html#PYTOOLS.torch.Einsum.torch_convert">[docs]</a>
  <span class="nd">@staticmethod</span>
  <span class="k">def</span> <span class="nf">torch_convert</span><span class="p">(</span><span class="n">a</span><span class="p">:</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span><span class="o">-&gt;</span><span class="nb">tuple</span><span class="p">[</span><span class="s1">&#39;Einsum&#39;</span><span class="p">,</span><span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">,</span><span class="nb">int</span><span class="p">],</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span><span class="nb">tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Converts a :class:`torch.nn.Linear` instance into an instance of this class with same behaviour (up to numerical instabilities). An additional output is a function :func:`test` which draws a random sample of the module input given batch size and sequence length, and returns a comparison of the two modules on that sample. Example::</span>

<span class="sd">   mod = torch.nn.Linear(42,17); mod_,test = Einsum.torch_convert(mod)</span>
<span class="sd">   assert mod_.sig == &#39;pq,bnp-&gt;bnq&#39; and mod_.weight.shape == (42,17)</span>
<span class="sd">   cmp = test(64,100) # generates sample(batch:64,length:100) and returns aligned fwd and bwd values</span>
<span class="sd">   assert all(torch.allclose(u,v,atol=1e-6) for u,v in cmp.values()) # increase atol if it fails</span>

<span class="sd">:param a: the instance to convert</span>
<span class="sd">:return: an equivalent :class:`Einsum` instance and its test function</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="c1">#--------------------------------------------------------------------------------------------------</span>
    <span class="k">def</span> <span class="nf">set_data</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">T</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span>
    <span class="k">def</span> <span class="nf">get_grad</span><span class="p">()</span><span class="o">-&gt;</span><span class="n">Generator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span><span class="kc">None</span><span class="p">,</span><span class="kc">None</span><span class="p">]:</span>
      <span class="k">yield</span> <span class="s1">&#39;weight&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">T</span>
      <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="k">yield</span> <span class="s1">&#39;bias&#39;</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span><span class="n">a</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">)</span>
    <span class="bp">self</span> <span class="o">=</span> <span class="n">Einsum</span><span class="p">(</span><span class="s1">&#39;pq,bnp-&gt;bnq&#39;</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span>
    <span class="n">set_data</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">B</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span><span class="n">M</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
      <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">M</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">in_features</span><span class="p">)</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">a</span><span class="p">(</span><span class="n">x</span><span class="p">);</span> <span class="n">y_</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># forward</span>
      <span class="n">a</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">();</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># reset all gradients</span>
      <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">();</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># back-propagation</span>
      <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;&#39;</span><span class="p">:(</span><span class="n">y</span><span class="p">,</span><span class="n">y_</span><span class="p">)}</span><span class="o">|</span><span class="p">{</span><span class="n">p</span><span class="p">:(</span><span class="n">u</span><span class="p">,</span><span class="n">u_</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span><span class="n">u_</span><span class="p">,</span><span class="n">u</span> <span class="ow">in</span> <span class="n">get_grad</span><span class="p">()}</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">,</span><span class="n">test</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Jean-Marc Andreoli.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>